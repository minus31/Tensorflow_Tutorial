{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feeding data to a model \n",
    "    \n",
    " - Queue runners\n",
    " - Dataset(TFRecord)\n",
    "\n",
    "### File I/O using queue runners \n",
    "\n",
    " - Dataset Class는 Image read 기능을 직접적으로 제공하지 않기 때문에, 하위 레벨 API 인 `queue runner`를 이용한다. \n",
    " - queue runners 는 일종의 thread runner \n",
    " - Loading 과 Session이 병렬적으로 동작한다. $\\rightarrow$ Loading Delay없이 연속적인 training이 가능하다. \n",
    " - Directory 내 모든 학습 데이터(eg. Images)들을 직접 load하여 train할 경우 유용!\n",
    " \n",
    "\n",
    "#### $\\sharp$ 데이터를 학습 시키는 두 가지 방법 \n",
    "\n",
    " 1 . Load $\\rightarrow$ Preprocessing $\\rightarrow$ train\n",
    "     > raw data를 로드하여 전처리를 수행 한 후 학습 \n",
    " \n",
    " 2 . Load $\\rightarrow$ Preprocessing $\\rightarrow$ write datasets $\\rightarrow$ load dataset $\\rightarrow$ train\n",
    "     > raw data를 미리 전처리하여 파일로 쓰고, 전처리된 데이터를 로드하여 학습\n",
    "\n",
    "> - 전처리 라인 길면, 학습의 성능이 저하 될 수 있다. \n",
    "> - 일반적으로는 전처리 파이프라인을 별도로 운영하고, 전처리된 데이터를 활용하지만, Case by Case이다. \n",
    "> - 각 데이터 포맷 별 로 to tfrecord transformer를 구축하여 트레인 시스템 운영해보겠다. \n",
    "\n",
    "\n",
    "### File I/O using Dataset(TFRecord)\n",
    "\n",
    "- Tensorflow에서 권장하는 record dataset \n",
    "- csv와 유사하게 Dataset 클래스로 쉽게 데이터를 로드 할 수 있다. \n",
    "- 실제로 file load는 한 번만 이루어지므로 효율적인 data read각 가능하다. \n",
    "- HDFS 활용 시 매우 효과적이다. \n",
    "\n",
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/MAC/.pyenv/versions/anaconda3-5.0.1/envs/python_ana/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "import os \n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "image_dir = \"./cnn_dataset/images\"\n",
    "age_dir = './cnn_dataset/age.csv'\n",
    "# Dataset 디렉토리에 있는 파일들의 리스트를 저장 \n",
    "imgname_list = [os.path.join(image_dir, name) for name in os.listdir(image_dir)]\n",
    "\n",
    "imgname_queue = tf.train.string_input_producer(imgname_list, num_epochs=1, shuffle=False,)\n",
    "agename_queue = tf.train.string_input_producer([age_dir], num_epochs=1,shuffle=False)\n",
    "\n",
    "img_reader = tf.WholeFileReader()\n",
    "age_reader = tf.TextLineReader()\n",
    "\n",
    "# key_img:파일이름, values_img:데이터  return \n",
    "key_img, raw_img = img_reader.read(imgname_queue)\n",
    "key_age, raw_age = age_reader.read(agename_queue)\n",
    "\n",
    "decoded_img = tf.image.decode_png(raw_img)\n",
    "decoded_age = tf.decode_csv(raw_age, [[0]])\n",
    "\n",
    "# gray image reshaping \n",
    "mean_reduced_image = tf.reduce_mean(decoded_img, axis=-1)\n",
    "reshaped_img = tf.reshape(mean_reduced_image, [-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 train data has been written\n",
      "500 train data has been written\n",
      "1000 train data has been written\n",
      "1500 train data has been written\n",
      "2000 train data has been written\n",
      "2500 train data has been written\n",
      "3000 train data has been written\n",
      "3500 train data has been written\n",
      "4000 train data has been written\n",
      "4500 train data has been written\n",
      "5000 train data has been written\n",
      "5500 train data has been written\n",
      "6000 test data has been written\n",
      "6500 test data has been written\n",
      "7000 test data has been written\n",
      "7500 test data has been written\n",
      "size of total dataset 7533\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess :\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    coord = tf.train.Coordinator()\n",
    "    thread = tf.train.start_queue_runners(sess, coord)\n",
    "\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "#     tf.initialize_all_variables()\n",
    "    \n",
    "    face_train_dir = './cnn_dataset/face_train.tfrecord'\n",
    "    face_test_dir = './cnn_dataset/face_test.tfrecord'\n",
    "    \n",
    "    train_writer = tf.python_io.TFRecordWriter(face_train_dir)\n",
    "    test_writer = tf.python_io.TFRecordWriter(face_test_dir)\n",
    "    \n",
    "    for i in range(99999999):\n",
    "        try :\n",
    "            _age, _img, _key = sess.run([decoded_age, reshaped_img, key_img])\n",
    "            \n",
    "            example = tf.train.Example()\n",
    "            example.features.feature['age'].int64_list.value.append(_age[0])\n",
    "            example.features.feature['img'].int64_list.value.extend(_img.tolist())\n",
    "            \n",
    "            if i < 6000:\n",
    "                train_writer.write(example.SerializeToString())\n",
    "                if i % 500 == 0:\n",
    "                    print('{} train data has been written'.format(i))\n",
    "            else:\n",
    "                test_writer.write(example.SerializeToString())\n",
    "                if i % 500 == 0 :\n",
    "                    print('{} test data has been written'.format(i))\n",
    "        \n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print('size of total dataset {}'.format(i))\n",
    "            break\n",
    "    \n",
    "    train_writer.close()\n",
    "    test_writer.close()\n",
    "    coord.request_stop()\n",
    "    coord.join(thread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "author": "",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
