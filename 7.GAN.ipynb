{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Network \n",
    "\n",
    " > - Generator와 Discriminator, 두 뉴럴넷이 서로 경쟁하면서 결과적으로 Discriminator의 결과가 0.5(구별 하지 못한다.)에 수렴하면서, Generator가 실제 데이터와 흡사한 데이터를 만들 수 가 있다.  \n",
    " \n",
    " - 여러 적용 사례 ; CycleGAN, StarGAN 등 ... \n",
    " \n",
    " \n",
    "###### Generator \n",
    "- 난수를 입력 받아, real image와 동일한 이미지를 출력하는 Neural Network \n",
    "- Discriminator 의 구분능렬이 좋을 수록 Generator의 loss가 증가 \n",
    "- 실제로 이 Generator를 학습 시켜서 이미지를 생성하는 모델을 만드는 것이 목표 \n",
    "\n",
    "###### Discriminator \n",
    "- fake image와 real image를 분류하는 Neural Network \n",
    "- fake image는 0, real_image는 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/MAC/.pyenv/versions/anaconda3-5.0.1/envs/python_ana/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variables in generator ------------\n",
      "<tf.Variable 'generator/layer1/kernel:0' shape=(10, 10) dtype=float32_ref>\n",
      "<tf.Variable 'generator/layer1/bias:0' shape=(10,) dtype=float32_ref>\n",
      "<tf.Variable 'generator/layer2/kernel:0' shape=(10, 10) dtype=float32_ref>\n",
      "<tf.Variable 'generator/layer2/bias:0' shape=(10,) dtype=float32_ref>\n",
      "<tf.Variable 'generator/layer3/kernel:0' shape=(10, 10) dtype=float32_ref>\n",
      "<tf.Variable 'generator/layer3/bias:0' shape=(10,) dtype=float32_ref>\n",
      "<tf.Variable 'generator/layerout/kernel:0' shape=(10, 10) dtype=float32_ref>\n",
      "<tf.Variable 'generator/layerout/bias:0' shape=(10,) dtype=float32_ref>\n",
      "variables in discriminator --------\n",
      "<tf.Variable 'discriminator/layer1/kernel:0' shape=(10, 10) dtype=float32_ref>\n",
      "<tf.Variable 'discriminator/layer1/bias:0' shape=(10,) dtype=float32_ref>\n",
      "<tf.Variable 'discriminator/layer2/kernel:0' shape=(10, 10) dtype=float32_ref>\n",
      "<tf.Variable 'discriminator/layer2/bias:0' shape=(10,) dtype=float32_ref>\n",
      "<tf.Variable 'discriminator/layer3/kernel:0' shape=(10, 10) dtype=float32_ref>\n",
      "<tf.Variable 'discriminator/layer3/bias:0' shape=(10,) dtype=float32_ref>\n",
      "<tf.Variable 'discriminator/layer4/kernel:0' shape=(10, 10) dtype=float32_ref>\n",
      "<tf.Variable 'discriminator/layer4/bias:0' shape=(10,) dtype=float32_ref>\n",
      "<tf.Variable 'discriminator/layeroutput/kernel:0' shape=(10, 1) dtype=float32_ref>\n",
      "<tf.Variable 'discriminator/layeroutput/bias:0' shape=(1,) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "def generator(z, reuse=False):\n",
    "    name = 'generator/layer{}'\n",
    "    layer1 = tf.layers.dense(z, 10, name=name.format(1), reuse=reuse)\n",
    "    layer2 = tf.layers.dense(layer1, 10, name=name.format(2), reuse=reuse)\n",
    "    layer3 = tf.layers.dense(layer2, 10, name=name.format(3), reuse=reuse)\n",
    "    output = tf.layers.dense(layer3, 10, name=name.format('out'), reuse=reuse)\n",
    "    return output\n",
    "\n",
    "def discriminator(feature, reuse=False):\n",
    "    name = 'discriminator/layer{}'\n",
    "    layer1 = tf.layers.dense(feature, 10,  name=name.format(1), reuse=reuse)\n",
    "    layer2 = tf.layers.dense(layer1, 10, name=name.format(2), reuse=reuse)\n",
    "    layer3 = tf.layers.dense(layer2, 10, name=name.format(3), reuse=reuse)\n",
    "    layer4 = tf.layers.dense(layer3, 10, name=name.format(4), reuse=reuse)\n",
    "    output = tf.layers.dense(layer4, 1, name=name.format('output'), reuse=reuse)\n",
    "    return output\n",
    "\n",
    "def gan_loss(logits_real, logits_fake):\n",
    "    loss_real = tf.losses.sigmoid_cross_entropy(tf.ones_like(logits_real), logits_real)\n",
    "    loss_fake = tf.losses.sigmoid_cross_entropy(tf.zeros_like(logits_fake), logits_fake)\n",
    "    discriminator_loss = loss_real + loss_fake\n",
    "    generator_loss = tf.losses.sigmoid_cross_entropy(tf.ones_like(logits_fake), logits_fake)\n",
    "    return generator_loss, discriminator_loss\n",
    "\n",
    "def train(loss, variables):\n",
    "    optimizer = tf.train.AdadeltaOptimizer(1e-2)\n",
    "    grads = optimizer.compute_gradients(loss, var_list=variables)\n",
    "    return optimizer.apply_gradients(grads)\n",
    "\n",
    "data = [[i for i in range(10)] for _ in range(1000)]\n",
    "normal = np.random.randn(1000, 10)\n",
    "\n",
    "real_data = tf.placeholder(tf.float32, [None, 10])\n",
    "z = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "fake_data = generator(z)\n",
    "disc_real_logits = discriminator(real_data, False)\n",
    "disc_fake_logits = discriminator(fake_data, True)\n",
    "\n",
    "disc_real_prob = tf.reduce_mean(tf.nn.sigmoid(disc_real_logits))\n",
    "disc_fake_prob = tf.reduce_mean(tf.nn.sigmoid(disc_fake_logits))\n",
    "\n",
    "loss_gen, loss_disc = gan_loss(disc_real_logits, disc_fake_logits)\n",
    "\n",
    "variables = tf.trainable_variables()\n",
    "\n",
    "gen_variables = [v for v in variables if v.name.startswith('generator')]\n",
    "disc_variables = [v for v in variables if v.name.startswith('discriminator')]\n",
    "\n",
    "print('variables in generator ------------')\n",
    "for v in gen_variables:\n",
    "    print(v)\n",
    "\n",
    "print('variables in discriminator --------')\n",
    "for v in disc_variables:\n",
    "    print(v)\n",
    "    \n",
    "disc_train_op = train(loss_disc, disc_variables)\n",
    "gen_train_op = train(loss_gen, gen_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:0 real_prob:0.012575588189065456, fake prob:0.49468445777893066 loss0.7289282083511353\n",
      "step:100 real_prob:0.015554897487163544, fake prob:0.4971252977848053 loss0.7206860780715942\n",
      "step:200 real_prob:0.01958148181438446, fake prob:0.49971333146095276 loss0.7124038338661194\n",
      "step:300 real_prob:0.025030065327882767, fake prob:0.5024200081825256 loss0.7042175531387329\n",
      "step:400 real_prob:0.03247089311480522, fake prob:0.5052425265312195 loss0.696163535118103\n",
      "step:500 real_prob:0.042734481394290924, fake prob:0.5081848502159119 loss0.688257098197937\n",
      "step:600 real_prob:0.05702781304717064, fake prob:0.5112552046775818 loss0.6805002689361572\n",
      "step:700 real_prob:0.07708130776882172, fake prob:0.5144596099853516 loss0.6728922128677368\n",
      "step:800 real_prob:0.10528557747602463, fake prob:0.5177991986274719 loss0.665417492389679\n",
      "step:900 real_prob:0.14468558132648468, fake prob:0.5212713479995728 loss0.6580614447593689\n",
      "generating data\n",
      "[ 0.07950938  0.24117716  0.35763004  0.12281908 -0.19235767  0.5670229\n",
      "  0.4958802  -1.2319144  -0.62625474 -0.2024222 ]\n",
      "[-0.0621338  -0.3386258  -0.08943774  0.29096788  0.2819586  -0.5528146\n",
      " -0.2658042   0.5211374  -0.00965422  0.19920005]\n",
      "[ 0.19540581  0.22079621 -0.11065289 -0.39834693 -0.5901253   0.24526267\n",
      "  0.3823353  -0.17759676  0.50530833 -0.0078805 ]\n",
      "[-0.5622086  -0.36510321 -0.15337986  0.62761164  0.13090895 -0.40976107\n",
      "  0.7080261   0.09802854  0.08938341  0.95664006]\n",
      "[-0.01626603 -0.24957322 -0.62725705  0.30616546 -1.0431969   0.64705956\n",
      "  1.6627991  -1.073903    0.55248225  0.35938025]\n",
      "[ 0.69131845 -0.2422354  -0.27517644  0.17475651 -0.32656172  0.40062866\n",
      " -0.08439324 -0.4465476   0.02764116 -0.80245966]\n",
      "[-0.7688845   0.24605231  0.5789112   0.1911073   0.6814946  -0.449641\n",
      "  0.02152362  0.18365136 -0.5107612   0.82752204]\n",
      "[ 1.1259217   0.46240368  0.3031512  -0.08981799  0.22302817  0.27094576\n",
      " -0.31162074  0.14019348  0.305326   -0.4261373 ]\n",
      "[-0.874206   -0.11209305 -0.10112351  0.14030157 -0.01204817 -0.29212785\n",
      "  0.8170326  -0.04466432  0.04525407  1.0163583 ]\n",
      "[ 0.04359492 -1.4433957  -1.7067015   0.20591252 -1.8304965   0.5392986\n",
      "  0.64026916 -0.8011523   0.6590196  -1.0214647 ]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(1000):\n",
    "        _, _real_prob, _fake_prob = sess.run([disc_train_op, disc_real_prob, disc_fake_prob],\n",
    "                                            feed_dict={real_data: data, z: normal})\n",
    "        \n",
    "        _, _loss = sess.run([gen_train_op, loss_gen], feed_dict={z:normal})\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(f'step:{i} real_prob:{_real_prob}, fake prob:{_fake_prob} loss{_loss}')\n",
    "            \n",
    "    _gen = sess.run(fake_data, feed_dict={z:normal})\n",
    "    print('generating data')\n",
    "    for g in _gen[:10]:\n",
    "        print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
